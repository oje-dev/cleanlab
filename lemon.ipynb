{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olly/Developer/cleanlab/lemon/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from cleanlab.classification import CleanLearning\n",
    "from cleanlab.lexical_quality.lexical_quality import LexicalQualityEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the original banking intent classification datataset from the cleanlab example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i accidentally made a payment to a wrong accou...</td>\n",
       "      <td>cancel_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i no longer want to transfer funds, can we can...</td>\n",
       "      <td>cancel_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancel my transfer, please.</td>\n",
       "      <td>cancel_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i want to revert this mornings transaction.</td>\n",
       "      <td>cancel_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just realised i made the wrong payment yeste...</td>\n",
       "      <td>cancel_transfer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            label\n",
       "0  i accidentally made a payment to a wrong accou...  cancel_transfer\n",
       "1  i no longer want to transfer funds, can we can...  cancel_transfer\n",
       "2                        cancel my transfer, please.  cancel_transfer\n",
       "3        i want to revert this mornings transaction.  cancel_transfer\n",
       "4  i just realised i made the wrong payment yeste...  cancel_transfer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://s.cleanlab.ai/banking-intent-classification.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to get a baseline reading of label accuracy to compare the accuracy score of the labels with and without our lexical filters or algorithm modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name google/electra-small-discriminator. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy of original model: 0.83\n"
     ]
    }
   ],
   "source": [
    "raw_texts, raw_labels = data[\"text\"].values, data[\"label\"].values\n",
    "raw_train_texts, raw_test_texts, raw_train_labels, raw_test_labels = train_test_split(raw_texts, raw_labels, test_size=0.1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(raw_train_labels)\n",
    "\n",
    "transformer = SentenceTransformer(\"google/electra-small-discriminator\")\n",
    "\n",
    "train_texts = transformer.encode(raw_train_texts)\n",
    "test_texts = transformer.encode(raw_test_texts)\n",
    "\n",
    "train_labels = encoder.transform(raw_train_labels)\n",
    "test_labels = encoder.transform(raw_test_labels)\n",
    "\n",
    "baseline_model = LogisticRegression(max_iter=400)\n",
    "baseline_model.fit(X=train_texts, y=train_labels)\n",
    "\n",
    "preds = baseline_model.predict(test_texts)\n",
    "baseline_og = accuracy_score(test_labels, preds)\n",
    "print(f\"\\n Test accuracy of original model: {baseline_og}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our new LexicalQualityEvaluator, we can add three columns to the dataframe.\n",
    "We add a spelling_score, quality_score and combined_lexical_score to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>spelling_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>combined_lexical_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i accidentally made a payment to a wrong accou...</td>\n",
       "      <td>cancel_transfer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972254</td>\n",
       "      <td>0.986127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i no longer want to transfer funds, can we can...</td>\n",
       "      <td>cancel_transfer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884922</td>\n",
       "      <td>0.942461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancel my transfer, please.</td>\n",
       "      <td>cancel_transfer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980948</td>\n",
       "      <td>0.990474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i want to revert this mornings transaction.</td>\n",
       "      <td>cancel_transfer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.352956</td>\n",
       "      <td>0.676478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just realised i made the wrong payment yeste...</td>\n",
       "      <td>cancel_transfer</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.904916</td>\n",
       "      <td>0.937752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            label  \\\n",
       "0  i accidentally made a payment to a wrong accou...  cancel_transfer   \n",
       "1  i no longer want to transfer funds, can we can...  cancel_transfer   \n",
       "2                        cancel my transfer, please.  cancel_transfer   \n",
       "3        i want to revert this mornings transaction.  cancel_transfer   \n",
       "4  i just realised i made the wrong payment yeste...  cancel_transfer   \n",
       "\n",
       "   spelling_score  quality_score  combined_lexical_score  \n",
       "0        1.000000       0.972254                0.986127  \n",
       "1        1.000000       0.884922                0.942461  \n",
       "2        1.000000       0.980948                0.990474  \n",
       "3        1.000000       0.352956                0.676478  \n",
       "4        0.970588       0.904916                0.937752  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"spelling_score\"] = [LexicalQualityEvaluator(text).get_spelling_score() for text in data[\"text\"]]\n",
    "data[\"quality_score\"] = [LexicalQualityEvaluator(text).get_quality_score() for text in data[\"text\"]]\n",
    "data[\"combined_lexical_score\"] = [LexicalQualityEvaluator(text).get_combined_quality_score() for text in data[\"text\"]]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use our new metrics to filter the data.\n",
    "For example, we can first sort the dataset by combined lexical score and then find any texts with a score of less than 0.6 and remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>spelling_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>combined_lexical_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>can i use top-up with apple pay?</td>\n",
       "      <td>apple_pay_or_google_pay</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.343864</td>\n",
       "      <td>0.600504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>i was spending cash with my card and got a fee.</td>\n",
       "      <td>card_payment_fee_charged</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.201074</td>\n",
       "      <td>0.600537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>how to activate google pay for top up?</td>\n",
       "      <td>apple_pay_or_google_pay</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.603550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>why was i charged extra when paying with card?</td>\n",
       "      <td>card_payment_fee_charged</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212546</td>\n",
       "      <td>0.606273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>i received my american express in apple pay, i...</td>\n",
       "      <td>apple_pay_or_google_pay</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.272377</td>\n",
       "      <td>0.609873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "893                   can i use top-up with apple pay?   \n",
       "232    i was spending cash with my card and got a fee.   \n",
       "871             how to activate google pay for top up?   \n",
       "219     why was i charged extra when paying with card?   \n",
       "930  i received my american express in apple pay, i...   \n",
       "\n",
       "                        label  spelling_score  quality_score  \\\n",
       "893   apple_pay_or_google_pay        0.857143       0.343864   \n",
       "232  card_payment_fee_charged        1.000000       0.201074   \n",
       "871   apple_pay_or_google_pay        0.875000       0.332100   \n",
       "219  card_payment_fee_charged        1.000000       0.212546   \n",
       "930   apple_pay_or_google_pay        0.947368       0.272377   \n",
       "\n",
       "     combined_lexical_score  \n",
       "893                0.600504  \n",
       "232                0.600537  \n",
       "871                0.603550  \n",
       "219                0.606273  \n",
       "930                0.609873  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_lexical_score_threshold = 0.6\n",
    "\n",
    "filtered_data = data.sort_values(by=\"combined_lexical_score\", ascending=True)\n",
    "\n",
    "unfiltered_size = data.size\n",
    "filtered_data = filtered_data[filtered_data[\"combined_lexical_score\"] >= combined_lexical_score_threshold]\n",
    "filtered_size = filtered_data.size\n",
    "\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295 texts with a combined lexical score of less than 0.6 removed from the dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"{unfiltered_size - filtered_size} texts with a combined lexical score of less than {combined_lexical_score_threshold} removed from the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a new regression model and compare the labelling accuracy score after applying lexical filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name google/electra-small-discriminator. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy of filtered model: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "filtered_raw_texts, filtered_raw_labels = filtered_data[\"text\"].values, filtered_data[\"label\"].values\n",
    "filtered_raw_train_texts, filtered_raw_test_texts, filtered_raw_train_labels, filtered_raw_test_labels = train_test_split(filtered_raw_texts, filtered_raw_labels, test_size=0.1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(filtered_raw_train_labels)\n",
    "\n",
    "transformer = SentenceTransformer(\"google/electra-small-discriminator\")\n",
    "\n",
    "filtered_train_texts = transformer.encode(filtered_raw_train_texts)\n",
    "filtered_test_texts = transformer.encode(filtered_raw_test_texts)\n",
    "\n",
    "filtered_train_labels = encoder.transform(filtered_raw_train_labels)\n",
    "filtered_test_labels = encoder.transform(filtered_raw_test_labels)\n",
    "\n",
    "filtered_model = LogisticRegression(max_iter=400)\n",
    "filtered_model.fit(X=filtered_train_texts, y=filtered_train_labels)\n",
    "\n",
    "filtered_preds = filtered_model.predict(filtered_test_texts)\n",
    "filtered_og = accuracy_score(filtered_test_labels, filtered_preds)\n",
    "print(f\"\\n Test accuracy of filtered model: {filtered_og}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline accuracy score was 0.83, with the filtering applied our accuracy score is now 0.74. Lexical filtering has decreased the label accuracy by -11.22%\n"
     ]
    }
   ],
   "source": [
    "percentage_change = ((filtered_og - baseline_og) / baseline_og) * 100\n",
    "print(f\"The baseline accuracy score was {baseline_og:.2f}, with the filtering applied our accuracy score is now {filtered_og:.2f}. Lexical filtering has {'increased' if percentage_change > 0 else 'decreased'} the label accuracy by {percentage_change:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we want to do is incorporate our combined lexical score into cleanlab's find label issues algorithm. But first we can use their algorithm without modifications to see the number of labels which are flagged as low quality, remove them from the dataset and then get another baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>given_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.969299</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.738626</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.285632</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.975937</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.102667</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_label_issue  label_quality  given_label  predicted_label\n",
       "0           False       0.969299            6                6\n",
       "1           False       0.738626            4                4\n",
       "2           False       0.285632            0                8\n",
       "3           False       0.975937            2                2\n",
       "4           False       0.102667            3                5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=400)\n",
    "\n",
    "cv_n_folds = 5\n",
    "cl = CleanLearning(model, cv_n_folds=cv_n_folds)\n",
    "\n",
    "label_issues = cl.find_label_issues(X=train_texts, labels=train_labels)\n",
    "label_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanlab found 41 potential label errors in the dataset.\n",
      "Here are indices of the top 10 most likely errors: \n",
      " [383 669 359 485 104  60 196 727 394 816]\n"
     ]
    }
   ],
   "source": [
    "identified_issues = label_issues[label_issues[\"is_label_issue\"] == True]\n",
    "lowest_quality_labels = label_issues[\"label_quality\"].argsort()[:10].to_numpy()\n",
    "\n",
    "print(\n",
    "    f\"cleanlab found {len(identified_issues)} potential label errors in the dataset.\\n\"\n",
    "    f\"Here are indices of the top 10 most likely errors: \\n {lowest_quality_labels}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of cleanlab's model: 0.82\n"
     ]
    }
   ],
   "source": [
    "cl.fit(X=train_texts, labels=train_labels, label_issues=cl.get_label_issues())\n",
    "\n",
    "pred_labels = cl.predict(test_texts)\n",
    "acc_cl = accuracy_score(test_labels, pred_labels)\n",
    "print(f\"Test accuracy of cleanlab's model: {acc_cl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our own modified find label issues algoritm with the lexical scoress argument flag to combine our combined lexical score with cleanlab's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of cleanlab's model with combined lexical scores: 0.83\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=400)\n",
    "\n",
    "cv_n_folds = 5\n",
    "cl = CleanLearning(model, cv_n_folds=cv_n_folds)\n",
    "\n",
    "lexical_scores = np.array([LexicalQualityEvaluator(text).get_combined_quality_score() for text in raw_train_texts])\n",
    "label_issues = cl.find_label_issues(X=train_texts, labels=train_labels, lexical_scores=lexical_scores)\n",
    "\n",
    "cl.fit(X=train_texts, labels=train_labels, label_issues=cl.get_label_issues())\n",
    "\n",
    "pred_labels = cl.predict(test_texts)\n",
    "combined_acc_cl = accuracy_score(test_labels, pred_labels)\n",
    "print(f\"Test accuracy of cleanlab's model with combined lexical scores: {combined_acc_cl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline accuracy score was 0.82, with the lexical score applied our accuracy score is now 0.83. The new algorithm has increased the label accuracy by 1.22%\n"
     ]
    }
   ],
   "source": [
    "percentage_change = ((combined_acc_cl - acc_cl) / acc_cl) * 100\n",
    "print(\n",
    "    f\"The baseline accuracy score was {acc_cl:.2f}, with the lexical score applied our accuracy score is now {combined_acc_cl:.2f}. The new algorithm has {'increased' if percentage_change > 0 else 'decreased'} the label accuracy by {percentage_change:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the tests suggest that the methods used to determine the spelling accuracy and lexical quality of the given texts are not very accurate in the first place. This combined with the simple approach of averaging the lexical quality score with cleanlab's confidence score resulted in little improvement.\n",
    "To further improve the accuracy, a better model could be used to more accurately determine the lexical quality of the given texts, as well as a more considered approach when incorporating these scores with the existing algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lemon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
